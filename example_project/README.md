# DFT Example Project

This example project demonstrates DFT (Data Flow Tools) capabilities with sample pipelines and configurations.

## ğŸ“ Project Structure

```
example_project/
â”œâ”€â”€ dft_project.yml          # Project configuration and database connections
â”œâ”€â”€ data/                    # Sample data files
â”‚   â””â”€â”€ sample_data.csv
â”œâ”€â”€ dft/                     # Custom components (plugin system)
â”‚   â”œâ”€â”€ sources/            # Custom data sources
â”‚   â”‚   â””â”€â”€ my_custom_source.py     # Example custom source
â”‚   â”œâ”€â”€ processors/         # Custom data processors
â”‚   â”‚   â””â”€â”€ my_custom_processor.py  # Example custom processor
â”‚   â””â”€â”€ endpoints/          # Custom data endpoints
â”‚       â””â”€â”€ my_custom_endpoint.py   # Example custom endpoint
â”œâ”€â”€ pipelines/               # Pipeline YAML definitions
â”‚   â”œâ”€â”€ analytics_pipeline.yml
â”‚   â”œâ”€â”€ base_data_pipeline.yml
â”‚   â”œâ”€â”€ custom_example_pipeline.yml # Uses custom components
â”‚   â”œâ”€â”€ customer_analysis.yml
â”‚   â”œâ”€â”€ daily_transactions.yml
â”‚   â”œâ”€â”€ independent_pipeline.yml
â”‚   â”œâ”€â”€ reporting_pipeline.yml
â”‚   â””â”€â”€ simple_csv_example.yml
â””â”€â”€ output/                  # Output files (auto-generated)
```

## ğŸš€ Quick Start

### 1. Install DFT

```bash
# Install from PyPI
pip install dft-pipeline

# Or from source (development)
# cd .. && pip install -e . && cd example_project
```

### 2. Run a Simple Example

```bash
# Run a basic CSV processing pipeline
dft run --select simple_csv_example

# Try the custom components example
dft run --select custom_example_pipeline
```

### 3. View Documentation

```bash
# Generate and serve interactive documentation
dft docs --serve
# Opens at http://localhost:8080
```

### 4. Explore Components

```bash
# List all available components
dft components list

# Get details about a specific component
dft components describe mysql

# View YAML examples
dft components describe validator --format yaml
```

## ğŸ”§ Example Pipelines

### simple_csv_example.yml
Basic CSV processing pipeline for testing without database dependencies.

### custom_example_pipeline.yml
**NEW**: Demonstrates the plugin system with custom sources, processors, and endpoints.

### analytics_pipeline.yml
Demonstrates pipeline dependencies (depends on `base_data_pipeline`).

### customer_analysis.yml
Shows complex data processing with multiple sources and validation steps.

### daily_transactions.yml
Example of daily batch processing with data validation.

## ğŸ’¾ Database Examples

The project includes examples for:
- **PostgreSQL** sources and endpoints with named connections
- **ClickHouse** endpoints with advanced configurations
- **MySQL** endpoints with upsert functionality
- **CSV** file processing
- **Custom Components** - plugin system for extending DFT with your own sources, processors, and endpoints

## ğŸ” Pipeline Features Demonstrated

- **Dependencies**: Pipeline execution order based on `depends_on`
- **Tags**: Grouping pipelines by purpose (`analytics`, `daily`, etc.)
- **Variables**: Template variables for flexible configurations
- **Validation**: Data quality checks with custom rules
- **Named Connections**: Reusable database connection definitions
- **Upsert Operations**: Insert or update operations for incremental loads
- **Plugin System**: Custom components that are automatically discovered and available in pipelines

## ğŸ“Š Documentation & Monitoring

### Interactive Documentation
- View all pipelines with dependencies
- Explore available components with configuration examples
- Interactive dependency graphs

### CLI Tools
```bash
dft validate          # Validate pipeline configurations
dft deps              # Show pipeline dependencies
dft components list   # List available components
```

## ğŸ¯ For Analysts

This example project shows how analysts can:

1. **Discover Components**: Use `dft components` to find sources, processors, and endpoints
2. **Copy Configurations**: Get ready-to-use YAML examples from documentation
3. **Build Pipelines**: Combine components with clear configuration patterns
4. **Create Custom Components**: Add your own sources/processors/endpoints to the `dft/` directory
5. **Validate Work**: Use built-in validation to catch configuration errors
6. **Monitor Dependencies**: Understand pipeline relationships and execution order

## ğŸ”— Next Steps

- Explore the interactive documentation at `dft docs --serve`
- Try the custom components example: `dft run --select custom_example_pipeline`
- Create your own custom components in the `dft/sources/`, `dft/processors/`, or `dft/endpoints/` directories
- Try modifying pipeline configurations
- Add your own database connections in `dft_project.yml`
- Create new pipelines using the component examples